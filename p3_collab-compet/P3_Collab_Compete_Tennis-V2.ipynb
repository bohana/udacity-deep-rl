{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Tennis (Collaboration and Competition)\n",
    "\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\", no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    \n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "            \n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Starts\n",
    "\n",
    "## Approach\n",
    "\n",
    "We will modify the `DDPG` implementation of the previous P2 assignment in order to satisfy the assumptions of `MADDPG`:\n",
    "* agents can share information while training their respective policies.\n",
    "* however at runtime, each agent acts independently, without knowledge of each other.\n",
    "\n",
    "To that end, we will make the following modifications:\n",
    "* During training each agent has access to a **shared** replay buffer.\n",
    "* The **critic** network will take into account states/actions from all agents during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Actor and Critic Networks\n",
    "\n",
    "The `Actor` network is as per origina, DDPG assignment, described here:\n",
    "* https://github.com/bohana/udacity-deep-rl/blob/master/p2_control/README.md\n",
    "\n",
    "The `Critic` network contains modifications in order to learn from other agent's experiences. Namely: it is intialized with an additional `n_agents` argument, in order to size the input dimensions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        #self.bn = nn.BatchNorm1d(state_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        #state = self.bn(state)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # the output for each state is a continous function expressed by tanh non linearilty\n",
    "        return torch.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "        Critic (Value) Model.\n",
    "        \n",
    "        On the multi-agent scenario (n_agents > 1), the critic also factors in states/actions from the other agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, n_agents, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size*n_agents, fc1_units)\n",
    "        # MADDPG - input to critic also takes into account \"other\" states and actions\n",
    "        self.fc2 = nn.Linear(fc1_units + (action_size)*n_agents, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.bn = nn.BatchNorm1d(state_size*n_agents)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        \"\"\"Forward pass - uses stats/actions from all agents\"\"\"\n",
    "        if BATCH_NORM:\n",
    "            state = self.bn(states)\n",
    "            \n",
    "        xs = F.relu(self.fcs1(states))\n",
    "        x = torch.cat((xs, actions), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer and OU Noise Process\n",
    "\n",
    "For multi-agent, the replay buffer stores additional information from the \"other\" agent. This is reflected in the extended experience tuple defintion:\n",
    "\n",
    "```python\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\",\n",
    "                                                                \"next_state\", \"done\",\n",
    "                                                                \"other_states\", \"other_actions\", \n",
    "                                                                \"other_next_states\"])\n",
    "       \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"MADDPG-friendly Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # For MADDPG, our experience buffer also keeps track of actions/states from the \"other\" agent\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\",\n",
    "                                                                \"other_states\", \"other_actions\", \"other_next_states\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done, others_state,\n",
    "            others_action, others_next_state):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done, others_state,\n",
    "                            others_action, others_next_state)\n",
    "        \n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        other_states = torch.from_numpy(np.vstack([e.other_states for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        other_actions = torch.from_numpy(np.vstack([e.other_actions for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        other_nexts = torch.from_numpy(np.vstack([e.other_next_states for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones, other_states, other_actions, other_nexts)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OU Noise\n",
    "\n",
    "The `OUNoise` class implements a random noise process based on Ornstein-Uhlenbeck process. The core idea is to provide randomness correlated with past randomly generated input.\n",
    "\n",
    "* See also: https://www.quora.com/Why-do-we-use-the-Ornstein-Uhlenbeck-Process-in-the-exploration-of-DDPG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "            \n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, buffer, random_seed, n_agents, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed,\n",
    "                                 fc1_units=fc1_units, fc2_units=fc2_units).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed,\n",
    "                                  fc1_units=fc1_units, fc2_units=fc2_units).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(),\n",
    "                                          lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size,\n",
    "                                   action_size,\n",
    "                                   random_seed,\n",
    "                                   n_agents,\n",
    "                                   fc1_units=fc1_units,\n",
    "                                   fc2_units=fc2_units).to(device)\n",
    "        \n",
    "        self.critic_target = Critic(state_size,\n",
    "                                    action_size,\n",
    "                                    random_seed,\n",
    "                                    n_agents,\n",
    "                                    fc1_units=fc1_units,\n",
    "                                    fc2_units=fc2_units).to(device)\n",
    "        \n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(),\n",
    "                                           lr=LR_CRITIC,\n",
    "                                           weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed, theta=NOISE_THETA, sigma=NOISE_SIGMA)\n",
    "        self.noise_eps = NOISE_EPS_START\n",
    "\n",
    "        # Replay memory - use shared cache passed as argument\n",
    "        self.memory = buffer\n",
    "        \n",
    "        self.train_counter = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, others_state,\n",
    "             others_action, others_next_state, agent_id):\n",
    "        \n",
    "        # when saving, make sure agent - \"other\" mapping is invariant\n",
    "        if agent_id == 0:\n",
    "            self.memory.add(state, action, reward, next_state, done,\n",
    "                            others_state, others_action, others_next_state)\n",
    "        else:\n",
    "            self.memory.add(others_state, others_action, reward, others_next_state, done,\n",
    "                            state, action, next_state)\n",
    "            \n",
    "        \n",
    "        self.train_counter += 1\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > (UPDATE_EVERY * BATCH_SIZE) and (self.train_counter >= UPDATE_EVERY):\n",
    "            for _ in range(LEARN_ROUNDS):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "            self.train_counter = 0\n",
    "            \n",
    "            self.noise_eps -= NOISE_EPS_DECAY\n",
    "            self.noise_eps = max(NOISE_EPS_MIN, self.noise_eps)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        \n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "            \n",
    "        self.actor_local.train()\n",
    "\n",
    "        if add_noise:\n",
    "            action += self.noise_eps * self.noise.sample()\n",
    "            \n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        \n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        \n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        (states, actions, rewards, next_states, dones, other_states, \n",
    "         other_actions, other_next_states) = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get batch predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        \n",
    "        # concatenate for critic features\n",
    "        states_all = torch.cat((states, other_states), dim=1).to(device)\n",
    "        actions_next_all = torch.cat((actions_next, other_actions), dim=1).to(device)\n",
    "        actions_all = torch.cat((actions, other_actions), dim=1).to(device)\n",
    "        \n",
    "        Q_targets_next = self.critic_target(states_all, actions_next_all)\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states_all, actions_all)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        #other_actions_pred = self.other.actor_local(other_states)\n",
    "        \n",
    "        actor_loss = -self.critic_local(states_all,\n",
    "                                        torch.cat((actions_pred, other_actions), dim=1).to(device)).mean()\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        # gradient cliping\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPGAgents:\n",
    "    def __init__(self, state_size, action_size, n_agents, seed):\n",
    "        self.n_agents = n_agents\n",
    "        \n",
    "        self.shared_buffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, SEED)\n",
    "\n",
    "        self.agents = [Agent(state_size=state_size,\n",
    "                             action_size=action_size,\n",
    "                             buffer=self.shared_buffer,\n",
    "                             random_seed=SEED,\n",
    "                             n_agents=n_agents,\n",
    "                             fc1_units=LAYER1_NODES,\n",
    "                             fc2_units=LAYER2_NODES) for _ in range(n_agents)]\n",
    "    \n",
    "    def reset(self):\n",
    "        for agent in self.agents:\n",
    "            agent.reset()\n",
    "    \n",
    "    def act(self, states):\n",
    "        actions = [agent.act(state) for agent, state in zip(self.agents, states)]\n",
    "        return actions\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        '''Process a step using Unity data\n",
    "           - Unity will send its states/actions/etc... as [n_agents x X] arrays\n",
    "             in this method, we unpack this array so we can distinguish this vs \"other\" agents\n",
    "        '''\n",
    "        \n",
    "        for i, e in enumerate(zip(self.agents, states, actions, rewards, next_states,\n",
    "                                  dones)):\n",
    "            \n",
    "            agent, state, action, reward, next_state, done = e\n",
    "            \n",
    "            # each state and action comprises all agents, filter which one to use\n",
    "            other_idx = np.array([o for o in range(self.n_agents) if o != i])[0]\n",
    "            others_state = states[other_idx]\n",
    "            others_action = actions[other_idx]\n",
    "            others_next_state = next_states[other_idx]\n",
    "            \n",
    "            # send this/other states/actions for the learning step\n",
    "            agent.step(state, action, reward, next_state, done, others_state,\n",
    "                       others_action, others_next_state, agent_id=i)\n",
    "    \n",
    "    def save(self):\n",
    "        for n, ag in enumerate(self.agents):\n",
    "            torch.save(ag.actor_local.state_dict(), 'checkpoint_ag{}_actor.pth'.format(n))\n",
    "            torch.save(ag.critic_local.state_dict(), 'checkpoint_ag{}_critic.pth'.format(n))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that for scoring, we use the same approach described in the Udacity \"benchmark implementation\", which states:\n",
    "\n",
    "```\n",
    "maximum score over both agents, for each episode, and the orange line shows the average score (after taking the maximum over both agents) over the next 100 episodes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maddpg_unity(env, mad_agent, n_eps=1000, max_t=1000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    eps_deque = deque(maxlen=print_every)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for eps in range(n_eps):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        episode_scores = np.zeros(NUM_AGENTS)\n",
    "        \n",
    "        mad_agent.reset()\n",
    "\n",
    "        for t in range(max_t):     \n",
    "            actions = mad_agent.act(states)\n",
    "            \n",
    "            # execute actions in unity\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            # learn\n",
    "            mad_agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            episode_scores += np.array(rewards)\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break \n",
    "        \n",
    "        score = episode_scores.max()\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        eps_deque.append(t + 1)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage of Max Scores: {:.3f}'.format(eps,\n",
    "                                                                   np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        if eps % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage of Max Scores: {:.3f}'.format(eps, np.mean(scores_deque)), end=\"\")\n",
    "            print('; Mean eps len: {:.2f}; Sum scores: {:.2f}; Noise eps: {:.3f}'.format(np.mean(eps_deque),\n",
    "                                                                                         np.sum(scores_deque),\n",
    "                                                                                         mad_agents.agents[0].noise_eps))\n",
    "            #print('Last actions:', actions)\n",
    "            \n",
    "        if np.mean(scores_deque) >= STOP_AT:\n",
    "            break\n",
    "    \n",
    "    mad_agents.save()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters\n",
    "\n",
    "Using noise decay suggestion from:\n",
    "https://towardsdatascience.com/training-two-agents-to-play-tennis-8285ebfaec5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 150000    # replay buffer size\n",
    "BATCH_SIZE = 100        # minibatch size\n",
    "GAMMA = 0.99           # discount factor\n",
    "TAU = 0.001            # for soft update of target parameters\n",
    "LR_ACTOR = 0.001      # learning rate of the actor \n",
    "LR_CRITIC = 0.001     # learning rate of the critic\n",
    "WEIGHT_DECAY = 0       # L2 weight decay\n",
    "\n",
    "N_EPS = 5000\n",
    "MAX_T = 1000\n",
    "\n",
    "UPDATE_EVERY = 15       # num of timesteps to run learning\n",
    "LEARN_ROUNDS = 10       # repeat learning (sample from memory + backprop) this many times \n",
    "\n",
    "LAYER1_NODES=300       # number of units in hidden layer 1 (both networks)\n",
    "LAYER2_NODES=100       # number of units in hidden layer 2 (both networks)\n",
    "\n",
    "#\n",
    "# noise parameters for OU process\n",
    "#\n",
    "NOISE_THETA=0.45\n",
    "NOISE_SIGMA=0.55\n",
    "\n",
    "# noise decay\n",
    "NOISE_EPS_START=1.0\n",
    "NOISE_EPS_MIN=1.0\n",
    "NOISE_EPS_DECAY=0.0\n",
    "\n",
    "\n",
    "BATCH_NORM = True\n",
    "STOP_AT = 0.5\n",
    "\n",
    "NUM_AGENTS = 2\n",
    "\n",
    "\n",
    "SEED = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your device: cuda:0\n",
      "- training starts Mon Mar 18 00:21:24 2019\n",
      "Episode 0\tAverage of Max Scores: 0.000; Mean eps len: 14.00; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 100\tAverage of Max Scores: 0.016; Mean eps len: 17.15; Sum scores: 1.63; Noise eps: 1.000\n",
      "Episode 200\tAverage of Max Scores: 0.013; Mean eps len: 17.80; Sum scores: 1.28; Noise eps: 1.000\n",
      "Episode 300\tAverage of Max Scores: 0.012; Mean eps len: 17.46; Sum scores: 1.16; Noise eps: 1.000\n",
      "Episode 400\tAverage of Max Scores: 0.011; Mean eps len: 17.88; Sum scores: 1.08; Noise eps: 1.000\n",
      "Episode 0\tAverage of Max Scores: 0.000; Mean eps len: 14.00; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 100\tAverage of Max Scores: 0.000; Mean eps len: 14.22; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 200\tAverage of Max Scores: 0.001; Mean eps len: 14.37; Sum scores: 0.09; Noise eps: 1.000\n",
      "Episode 300\tAverage of Max Scores: 0.000; Mean eps len: 14.20; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 400\tAverage of Max Scores: 0.000; Mean eps len: 14.21; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 500\tAverage of Max Scores: 0.001; Mean eps len: 14.46; Sum scores: 0.10; Noise eps: 1.000\n",
      "Episode 600\tAverage of Max Scores: 0.000; Mean eps len: 14.20; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 700\tAverage of Max Scores: 0.000; Mean eps len: 14.18; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 800\tAverage of Max Scores: 0.001; Mean eps len: 14.38; Sum scores: 0.10; Noise eps: 1.000\n",
      "Episode 900\tAverage of Max Scores: 0.002; Mean eps len: 14.69; Sum scores: 0.20; Noise eps: 1.000\n",
      "Episode 1000\tAverage of Max Scores: 0.000; Mean eps len: 14.20; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 1100\tAverage of Max Scores: 0.001; Mean eps len: 14.39; Sum scores: 0.09; Noise eps: 1.000\n",
      "Episode 1200\tAverage of Max Scores: 0.001; Mean eps len: 14.36; Sum scores: 0.09; Noise eps: 1.000\n",
      "Episode 1300\tAverage of Max Scores: 0.000; Mean eps len: 14.36; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 1400\tAverage of Max Scores: 0.003; Mean eps len: 15.31; Sum scores: 0.29; Noise eps: 1.000\n",
      "Episode 1500\tAverage of Max Scores: 0.001; Mean eps len: 14.35; Sum scores: 0.10; Noise eps: 1.000\n",
      "Episode 1600\tAverage of Max Scores: 0.001; Mean eps len: 14.33; Sum scores: 0.10; Noise eps: 1.000\n",
      "Episode 1700\tAverage of Max Scores: 0.000; Mean eps len: 14.20; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 1800\tAverage of Max Scores: 0.001; Mean eps len: 14.40; Sum scores: 0.10; Noise eps: 1.000\n",
      "Episode 1900\tAverage of Max Scores: 0.000; Mean eps len: 14.20; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 2000\tAverage of Max Scores: 0.001; Mean eps len: 14.40; Sum scores: 0.10; Noise eps: 1.000\n",
      "Episode 2100\tAverage of Max Scores: 0.000; Mean eps len: 14.20; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 2200\tAverage of Max Scores: 0.001; Mean eps len: 14.42; Sum scores: 0.10; Noise eps: 1.000\n",
      "Episode 2300\tAverage of Max Scores: 0.001; Mean eps len: 14.52; Sum scores: 0.10; Noise eps: 1.000\n",
      "Episode 2400\tAverage of Max Scores: 0.000; Mean eps len: 14.38; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 2500\tAverage of Max Scores: 0.002; Mean eps len: 14.71; Sum scores: 0.20; Noise eps: 1.000\n",
      "Episode 2600\tAverage of Max Scores: 0.000; Mean eps len: 14.20; Sum scores: 0.00; Noise eps: 1.000\n",
      "Episode 2614\tAverage of Max Scores: 0.000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b34d6210a3cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                        \u001b[0mn_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                        \u001b[0mmax_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_T\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                        print_every=100)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n- training end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masctime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a2b79703e270>\u001b[0m in \u001b[0;36mmaddpg_unity\u001b[0;34m(env, mad_agent, n_eps, max_t, print_every)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mmad_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a76bd4270d75>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# send this/other states/actions for the learning step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             agent.step(state, action, reward, next_state, done, others_state,\n\u001b[0;32m---> 42\u001b[0;31m                        others_action, others_next_state, agent_id=i)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-db0ba04075a2>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done, others_state, others_action, others_next_state, agent_id)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLEARN_ROUNDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-db0ba04075a2>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Minimize the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;31m# gradient cliping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdev/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdev/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "print('Your device:', device)\n",
    "print('- training starts', time.asctime())\n",
    "\n",
    "mad_agents = MADDPGAgents(state_size, action_size, NUM_AGENTS, SEED)\n",
    "scores = []\n",
    "\n",
    "# initially, switch off noise decay and populate the replay buffer - this step does not do any learning\n",
    "NOISE_EPS_DECAY=0.0\n",
    "UPDATE_EVERY=500\n",
    "scores += maddpg_unity(env, mad_agents,\n",
    "                       n_eps=500,\n",
    "                       max_t=MAX_T,\n",
    "                       print_every=100)\n",
    "\n",
    "\n",
    "# reset to normal behavior: learn and decay ou noise\n",
    "UPDATE_EVERY=15\n",
    "\n",
    "scores += maddpg_unity(env, mad_agents,\n",
    "                       n_eps=N_EPS,\n",
    "                       max_t=MAX_T,\n",
    "                       print_every=100)\n",
    "\n",
    "print('\\n\\n- training end', time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.figure.set_size_inches(12, 5)\n",
    "\n",
    "plt.plot(np.arange(1, len(scores)+1),\n",
    "         scores)\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "s = pd.Series(scores)\n",
    "r = s.rolling(100)\n",
    "\n",
    "r.mean().dropna().plot(figsize=(12, 5), title='Rolling avg 100eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning notes\n",
    "\n",
    "* no improvements: learning stuck due to low exploration -> tweak noise parameters\n",
    "* training colapse: too much exploration?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
